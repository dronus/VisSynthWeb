<!DOCTYPE  html>
<html>
<head>
  <title>NF VisSynth</title>
  <script src="OES_texture_float_linear-polyfill.js"></script>  
  <script src="spline.js"></script>
  <script src="texture.js"></script>
  <script src="shader.js"></script>
  <script src="glmatrix.js"></script>
  <script src="audio.js"></script>
  <script src="canvas.js"></script>  
  <script src="filters.js"></script>
</head>
<body>
  <style>
    html,body,canvas{
      width: 100%;
      height: 100%;
      padding: 0px;
      margin: 0px;
      overflow: hidden;
      background-color: #000;
    }
  </style>

  <script>

    // "canvas" is the global WebGL effect canvas object, defined by canvas.js

    // append the canvas to the view
    document.body.appendChild(canvas);
        
    // get session url, if any
    var session_url='/';
    if(document.location.hash)
      session_url+=document.location.hash.substring(1)+'_';

    // establish WebSocket connection to command server
    var websocket;
    var open_socket=function()
    {
      websocket=new WebSocket('ws://'+document.location.hostname+':'+document.location.port);
      websocket.onopen=function(){
        // opt in for commands
        websocket.send(JSON.stringify({'method':'get', path:'/feeds'+session_url+'command',data:''}));    
      };
      // opt-in for command feed from remote control server
      websocket.onmessage=function(event)
      {
        var packet=JSON.parse(event.data);
        var path=packet.path, message=packet.data;
        
        if(path=='/feeds'+session_url+'command')
        {
          var js=message;
          var result= window.eval(js);
          if(result){
            put('result',JSON.stringify(result));
          }
        }
      }
      websocket.onclose=function()
      {
        setTimeout(open_socket,1000);
      }
    }
    open_socket();

    var put=function(path,data){
      if(websocket.readyState)
        websocket.send(JSON.stringify({'method':'put', path:'/feeds'+session_url+path,data:data}));
    }

    var time=0,frame_time=0; // running time
    var preview_cycle=0;
    var preview_enabled=false;
    var screenshot_cycle=0;
    var preview_canvas=null;
    var chain=null;

    // main update function, shows video frames via glfx.js canvas
    var update = function()
    {    
      // enqueue next update
      if(canvas.proposed_fps)
        setTimeout(function(){
          requestAnimationFrame(update);
        },1000/canvas.proposed_fps);
      else
        requestAnimationFrame(update);
    
      // quirk: glfx.js canvas may not be initalized if draw was not called.
      if(!canvas._.isInitialized)
        canvas.initialize(800,600);

      // get animation time
      var current_time=Date.now();
      frame_time=frame_time*0.9 + (current_time-time)*0.1;
      time=current_time;
      var effect_time=time*0.001; // 1 units per second      

      // run effect chain
      random_index=0; // used by effect chain to distinguish all random invocations in a single frame
      if(chain) chain(canvas,effect_time);

      // provide preview if requested
      // the preview is a downscaled image provided by the 'preview' effect
      // we crop the preview pixels of the canvas just BEFORE canvas.update, which will redraw the full resolution canvas.
      //
      // in repsect to just downsizing the final image this has two benefits:
      //
      // 1) it is much faster, as rescaling is done in WebGL context and not by 2d context drawImage
      //
      // 2) The 'preview' filter may be added to any chain position manually to tap the preview image between effects
      //
      if(preview_enabled && preview_cycle==1)
      {      
        if(!preview_canvas)
        {
          preview_canvas=document.createElement('canvas');
          preview_canvas.width=canvas.preview_width; preview_canvas.height=canvas.preview_height;
        }
        var ctx=preview_canvas.getContext('2d');
        ctx.drawImage(canvas,0,canvas.height-canvas.preview_height,canvas.preview_width,canvas.preview_height, 0, 0, canvas.preview_width,canvas.preview_height);
      }
      else if(preview_cycle==0)
      {          
        var jpeg=preview_enabled ? preview_canvas.toDataURL('image/jpeg') : null;
        var data={frame_time:frame_time, jpeg:jpeg};
        var json=JSON.stringify(data);
        put('preview',json);

        // only provide data every other frame if a preview image is send.
        // if only frame rate data is send, we keep the network calm.
        preview_cycle=preview_enabled ? 2 : 15; 
      }
      preview_cycle--;

      // redraw visible canvas
      canvas.update();

      // reset switched flag, it is used by some filters to clear buffers on chain switch
      canvas.switched=false; 
      
      // take screenshot if requested
      if(screenshot_cycle==1)
      {      
        var pixels=canvas.toDataURL('image/jpeg');    
        put('screenshot',pixels);
        screenshot_cycle=0;
      }
    };

    // enumerate the available sources at startup and start update loop if found
    var source_ids={audio:[],video:[]};
    function onSourcesAcquired(sources) 
    {
      for (var i = 0; i != sources.length; ++i) {      
        var source = sources[i];
        source_ids[source.kind].push(source.id);
      }      
      // start frequent canvas updates
      update();
    }
    if(MediaStreamTrack.getSources)
      MediaStreamTrack.getSources(onSourcesAcquired);
    else
      onSourcesAcquired([]);

    // let the remote change the audio source
    // 
    // this is a global function that is therefore available as a setup function in the filter chain
    // TODO find a cleaner way to provide this function
    //    
    var audio_device_index=0;
    function select_audio(device_index)
    {
      audio_device_index=parseInt(device_index);
    }

    // get the video feed from a capture device name by source_index into source_ids
    // opens the capture device and starts streaming on demand
    // the consumer may receive a still starting <video> object, so it has to add and handle 'canplay' event before properties like videoWidth are read.
    //
    // TODO this also grabs one audio source, selected by audio_device_index if not already done.
    // this should be done somewhere else (eg. audio depending filters) but if we don't do it now, 
    // the user may be asked twice, first for camera, then for microphone.
    // There is no clean solution to this, as any chain may use any audio / video source... 
    // maybe we should at least delay device grabbing until the pre_chain is evaluated for the first time,
    // we can then ask for the sources that will most likely be the only ones for the full session.
    var videos={};
    var audio_found=false;
    var get_video=function(source_index,width,height)
    {
      source_index = source_index | 0;

      // just return video, if already started
      if(videos[source_index]) 
        return videos[source_index];

      console.log("Acquire stream for device index "+source_index);

      // create a new <video> element for decoding the capture stream
      var video = document.createElement('video');
      videos[source_index]=video;
    
      var constraints = {
        video:
        {
          optional: [{sourceId: source_ids.video[source_index]}]
        },
        audio:false
      };

      // enforce resolution, if asked to
      if(width && height)
      {
          constraints.video.optional.width=width;  constraints.video.optional.height=height;
      }
      
      // TODO handle audio again, in a more flexible fashion than one device only...
      // for now, we just take the first device queried
      if(!audio_found)
      {
        constraints.audio={optional:[{sourceId:source_ids.audio[audio_device_index]}]};
        audio_found=true;
      }

      // initalize getUserMedia() camera capture
      var getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.oGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;        
      getUserMedia.call(navigator, constraints, function(stream){
      
        console.log("Got camera!");
        // capture device was successfully acquired
        video.autoplay = true;
        video.muted=true;
        if (video.mozSrcObject !== undefined) 
          video.mozSrcObject = stream;
        else
          video.src = URL.createObjectURL(stream);      

        // TODO handle audio again, in a more flexible fashion than one device only...
        if(stream.getAudioTracks().length) initAudioAnalysers(stream);
        
      }, function(err){
        console.log(err);
      });
    }
    
    // set video handler. 
    // the video devices are started on demand.
    // returns a <video> Element streaming the selected device.
    // this is used by the 'capture' effect in glfx.js to acquire the camera.
    canvas.video_source=get_video;


    // helper functions for chain code generation

    // flatten tree-like objects values into single array, dropping keys
    var flatten=function(o)
    {
      var a=[];
      for(var key in o) 
        if(key!='type') 
        {
          var value=o[key];
          if(typeof(value)=='object')
            a=a.concat(generators[value.type](value));
          else
            a.push(value);
        }
      return a;
    }

    // effect argument value generators...    
    var random_index=0;
    oscillators={
      sine  : Math.sin,
      saw   : function(t){return (t % (2*Math.PI))/Math.PI-1.},
      square: function(t,d){return (t % (2*Math.PI)<Math.PI*2.*d) ? 1. : -1.},
      random: function(t){
        t=t/Math.PI;
        var i0=Math.floor(t);
        var i1=i0+1;
        t=t-i0;
        var p1=42841, p2=99991;
        var r0=Math.sin(i0*p1+random_index*p2)+1.0;
        r0=(p1*r0) % 2.0 - 1.0;
        var r1=Math.sin(i1*p1+random_index*p2)+1.0;
        r1=(p1*r1) % 2.0 - 1.0;
        random_index++;
        t=(Math.sin((t-0.5)*Math.PI)+1.0)/2.0;
        return r0*(1.-t)+r1*t;
      }
    }
    
    var generators={
      perspective:function(args){delete args.type; return ["[-0.75,-0.75, -0.75,0.75, 0.75,-0.75, 0.75,0.75],["+flatten(args).join(',')+"],true"]},
      osc:function(args){return [args.a+"*oscillators['"+(args.waveform ? args.waveform : 'sine' )+"'](t*"+args.f+"+"+args.p+","+(args.duty?args.duty:0.5)+")+"+args.o]},
      pos:function(args){return [args.x+"+w/2",args.y+"+h/2"]},
      beat:function(args){return["audio_engine.beatValue("+flatten(args).join(',')+")"]},
      size:flatten,
      rgb:flatten,
      rgba:flatten,
      rgb_range:flatten
    };

    // generate a single effect function call source code
    var generate_code=function(effect)
    {
      var command=effect.effect;
      var args=[];
      for(var key in effect)
      {
        if(key=='effect') continue;
        var value=effect[key];
        if(typeof(value)=='object')
          args=args.concat(generators[value.type](value));
        else if (typeof(value)=='string')
          args.push('"'+value+'"');
        else
          args.push(value);
      }
      
      var src=command+'('+args.join(',')+'); ';
            
      return src;
    }

    // global functions called by remote control

    // set effect chain to render
    function setChain(effects)
    {
      effects.unshift({'effect':'stack_prepare'});    
      var havePreview=false;
      for(var i=0; i<effects.length; i++)
        if(effects[i].effect=='preview')
          havePreview=true;
      if(!havePreview)
        effects.push({'effect':'preview'});
      // var effects=JSON.parse(json);
      
      // create JS chain function source code
      var src='var _=function(c,t){with(c){';
      for (var i=0; i<effects.length; i++)
        if(typeof(effects[i])!='string')  // skip names
          src+=generate_code(effects[i]);
      src+='}};_';

      // compile chain
      var w=canvas.width, h=canvas.height; // provide global values for chain functions
      chain=eval(src);
      
      // set canvas 'switched' flag, that can be used by filters to reset buffers
      canvas.switched=true;
    }
        
    // receive preview request from remote       
    function preview(enabled)
    {
      // engage preview process
      preview_enabled=enabled;
      preview_cycle=2;
    }
    
    // receive screenshot request from remote
    function screenshot()
    {
      // engage screenshot process
      screenshot_cycle=1;
    }
    
    // load startup chain (first three of chains.json : setup pre, current, setup after)
    var xmlHttp = new XMLHttpRequest();
    xmlHttp.open('GET','saves'+session_url+'chains.json',false);
    xmlHttp.send(null);
    if(xmlHttp.responseText)
    { 
      var chains=JSON.parse(xmlHttp.responseText);
      var full_chain=chains[0].concat(chains[2],chains[1]);
      setChain(full_chain);
    }
  
  </script>

</body>
</html>
