<!DOCTYPE  html>
<html>
<head>
  <title>NF VisSynth</title>
  <script src="OES_texture_float_linear-polyfill.js"></script>  
  <script src="spline.js"></script>
  <script src="texture.js"></script>
  <script src="shader.js"></script>
  <script src="glmatrix.js"></script>
  <script src="audio.js"></script>
  <script src="canvas.js"></script>  
  <script src="filters.js"></script>
</head>
<body>
<!--  <canvas id=audiocanvas width=640 height=256 style="position: absolute; width: 640px; height: 256px; border: 1px solid pink;">audio test canvas</canvas> -->
  <style>
    html,body,canvas{
      width: 100%;
      height: 100%;
      padding: 0px;
      margin: 0px;
      overflow: hidden;
      background-color: #000;
    }
  </style>

  <script>

    // "canvas" is the global glfx.js WebGL effect canvas object, defined by canvas.js

    // append the canvas to the view
    document.body.appendChild(canvas);
        
    // get session url, if any
    var session_url='/';
    if(document.location.hash)
      session_url+=document.location.hash.substring(1)+'_';
    
    var put=function(url,data){
      var xmlHttp = new XMLHttpRequest();
      xmlHttp.open('PUT','/feeds'+session_url+url,true);  
      xmlHttp.send(data);
    }

    var time=0,frame_time=0; // running time
    var preview_cycle=0;
    var preview_enabled=false;
    var screenshot_cycle=0;
    var preview_canvas=null;
    var chain=null;

    // main update function, shows video frames via glfx.js canvas
    var update = function()
    {
      // quirk: glfx.js canvas may not be initalized if draw was not called.
      if(!canvas._.isInitialized)
        canvas.initialize(800,600);

      // get animation time
      var current_time=Date.now();
      frame_time=frame_time*0.9 + (current_time-time)*0.1;
      time=current_time;
      var effect_time=time*0.001; // 1 units per second      

      // run effect chain
      if(chain) chain(canvas,effect_time);

      // provide preview if requested
      if(preview_enabled && preview_cycle==0)
      {      
        if(!preview_canvas)
        {
          preview_canvas=document.createElement('canvas');
          preview_canvas.width=canvas.preview_width; preview_canvas.height=canvas.preview_height;
        }
        var ctx=preview_canvas.getContext('2d');
        ctx.drawImage(canvas,0,canvas.height-canvas.preview_height,canvas.preview_width,canvas.preview_height, 0, 0, canvas.preview_width,canvas.preview_height);
      }
      else if(preview_cycle==1)
      {          
        var jpeg=preview_enabled ? preview_canvas.toDataURL('image/jpeg') : null;
	var data={frame_time:frame_time, jpeg:jpeg};
	var json=JSON.stringify(data);
        put('preview',json);
      }
      preview_cycle=(preview_cycle+1)%2;

      // redraw visible canvas
      canvas.update();

      // take screenshot if requested
      if(screenshot_cycle==1)
      {      
        var pixels=canvas.toDataURL('image/jpeg');    
        put('screenshot',pixels);
        screenshot_cycle=0;
      }

      // enqueue next update
      requestAnimationFrame(update);
    };

    // enumerate the available sources at startup and start update loop if found
    var source_ids={audio:[],video:[]};
    function onSourcesAcquired(sources) 
    {
      for (var i = 0; i != sources.length; ++i) {      
        var source = sources[i];
        source_ids[source.kind].push(source.id);
      }      
      // start frequent canvas updates
      update();
    }
    if(MediaStreamTrack.getSources)
      MediaStreamTrack.getSources(onSourcesAcquired);
    else
      onSourcesAcquired([]);


    // let the remote change the source device mapping
    var device_mapping={};
    function map_video(source,target)
    {
      device_mapping[parseInt(source)]=parseInt(target);
    }
    
    var audio_device_index=0;
    function select_audio(device_index)
    {
      audio_device_index=parseInt(device_index);
    }

    // get the video feed from a capture device name by source_index into source_ids
    // opens the capture device and starts streaming on demand
    // the consumer may receive a still starting <video> object, so it has to add and handle 'canplay' event before properties like videoWidth are read.
    var videos={};
    var audio_found=false;
    var get_video=function(source_index)
    {
      source_index = source_index | 0;

      /*
      var device_mapping=document.location.hash.substring(1);
      if(device_mapping) device_mapping=JSON.parse(device_mapping);
      */
      if(device_mapping[source_index]) 
        source_index=device_mapping[source_index];
        
      
      // jsut return video, if already started
      if(videos[source_index]) 
        return videos[source_index];

      console.log("Acquire stream for device index "+source_index);

      // create a new <video> element for decoding the capture stream
      var video = document.createElement('video');
      videos[source_index]=video;
    
      var constraints = {
        video:{
          optional: [{sourceId: source_ids.video[source_index]}]
        },
        audio:false
      };

      
      // TODO handle audio again, in a more flexible fashion than one device only...
      // for now, we just take the first device queried
      if(!audio_found)
      {
        constraints.audio={optional:[{sourceId:source_ids.audio[audio_device_index]}]};
        audio_found=true;
      }

      // initalize getUserMedia() camera capture
      var getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.oGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;        
      getUserMedia.call(navigator, constraints, function(stream){
      
        console.log("Got camera for id "+constraints.video.optional[0].sourceId);
        // capture device was successfully acquired
        video.autoplay = true;
        video.muted=true;
        if (video.mozSrcObject !== undefined) 
          video.mozSrcObject = stream;
        else
          video.src = URL.createObjectURL(stream);      
        // add our startup code to canplay handler of <video> object

        // TODO how to handle multiple devices with different resolutions?
        // for now, we just keep the inital resolution und don't adapt to devices at all!
        /*
        video.addEventListener('canplay',function(e){
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
        },false);
        */

        // TODO handle audio again, in a more flexible fashion than one device only...
        if(stream.getAudioTracks().length) initAudioAnalysers(stream);
        
      }, function(err){
        console.log(err);
      });
    }
    
    // set video handler. 
    // the video devices are started on demand.
    // returns a <video> Element streaming the selected device.
    // this is used by the 'capture' effect in glfx.js to acquire the camera.
    canvas.video_source=get_video;


    // command polling from remote control server
    var poll_command=function()
    {
      var xmlHttp = new XMLHttpRequest();
      xmlHttp.open('GET', '/feeds'+session_url+'command', true);
      xmlHttp.onreadystatechange = function () {
        if (xmlHttp.readyState == 4) {
          setTimeout(function(){
            poll_command();
          },100);
          var js=xmlHttp.responseText;
          if(js) 
          {
            var result=window.eval(js);
            if(result){
              put('result',JSON.stringify(result));
            }
          }
        }
      };
      xmlHttp.send(null);
    }
    poll_command();


    // helper functions for chain code generation

    // flatten tree-like objects values into single array, dropping keys
    var flatten=function(o){
      var a=[];
      for(var key in o) 
        if(key!='type') 
        {
          var value=o[key];
          if(typeof(value)=='object')
            a=a.concat(generators[value.type](value));
          else
            a.push(value);
        }
      return a;
    }
    // effect argument value generators...
    var generators={
      perspective:function(args){delete args.type; return ["[-0.75,-0.75, -0.75,0.75, 0.75,-0.75, 0.75,0.75],["+flatten(args).join(',')+"],true"]},
      osc:function(args){return [args.a+"*Math.sin(t*"+args.f+"+"+args.p+")+"+args.o]},
      pos:function(args){return [args.x+"+w/2",args.y+"+h/2"]},
      beat:function(args){return["audio_engine.beatValue("+flatten(args).join(',')+")"]},
      size:flatten,
      rgb:flatten,
      rgba:flatten,
      rgb_range:flatten
    };

    // generate a single effect function call source code
    var generate_code=function(effect)
    {
      var command=effect.effect;
      var args=[];
      for(var key in effect)
      {
        if(key=='effect') continue;
        var value=effect[key];
        if(typeof(value)=='object')
          args=args.concat(generators[value.type](value));
        else
          args.push(value);
      }
      
      var src=command+'('+args.join(',')+'); ';
            
      return src;
    }

    // global functions called by remote control

    // set effect chain to render
    function setChain(effects)
    {
      effects.unshift({'effect':'stack_prepare'});    
      var havePreview=false;
      for(var i=0; i<effects.length; i++)
        if(effects[i].effect=='preview')
          havePreview=true;
      if(!havePreview)
        effects.push({'effect':'preview'});
      // var effects=JSON.parse(json);
      
      // create JS chain function source code
      var src='var _=function(c,t){with(c){';
      for (var i=0; i<effects.length; i++)
        if(typeof(effects[i])!='string')  // skip names
          src+=generate_code(effects[i]);
      src+='}};_';

      // compile chain
      var w=canvas.width, h=canvas.height; // provide global values for chain functions
      chain=eval(src);
    }
    
    // receive preview request from remote       
    function preview(enabled)
    {
      // engage preview process
      preview_enabled=enabled;
      preview_cycle=0;
    }
    
    // receive screenshot request from remote
    function screenshot()
    {
      // engage screenshot process
      screenshot_cycle=1;
    }
    
    // load startup chain (first three of chains.json : setup pre, current, setup after)
    var xmlHttp = new XMLHttpRequest();
    xmlHttp.open('GET','saves'+session_url+'chains.json',false);
    xmlHttp.send(null);
    if(xmlHttp.responseText)
    { 
      var chains=JSON.parse(xmlHttp.responseText);
      var full_chain=chains[0].concat(chains[2],chains[1]);
      setChain(full_chain);
    }
    
  </script>

</body>
</html>
